{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aef1b773-651c-48d0-9506-6d5ed0b6c7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the library \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import tensorflow as tf \n",
    "import requests \n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3e63409-0281-4864-aca9-0b151a60b39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic stock_prices.csv created and loaded.\n"
     ]
    }
   ],
   "source": [
    "# Set up an environment \n",
    "\n",
    "# Create sample data set\n",
    "np.random.seed(42)\n",
    "data_length = 2000 # adjust length as needed. \n",
    "trend = np.linspace(100,200, data_length)\n",
    "noise = np.random.normal(0,2, data_length)\n",
    "synthetic_data = trend + noise \n",
    "\n",
    "#Create DF and save as stock_price.csv\n",
    "data = pd.DataFrame(synthetic_data, columns = ['Close'])\n",
    "data.to_csv('stock_price.csv', index = False)\n",
    "print(\"Synthetic stock_prices.csv created and loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9b0128b-5351-4660-b88c-9083d6ccdc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (1899, 100)\n",
      "Shape of Y: (1899,)\n"
     ]
    }
   ],
   "source": [
    "# Load the data set \n",
    "data = pd.read_csv('stock_price.csv')\n",
    "data = data[['Close']].values \n",
    "\n",
    "# Normalize the data \n",
    "scaler = MinMaxScaler(feature_range=(0,1)) # method use to normalize data \n",
    "data = scaler.fit_transform(data) \n",
    "\n",
    "# Prepare the data for training \n",
    "# use create_dataset to prepare the data for training \n",
    "def create_dataset(data, time_step=1):\n",
    "    x,y = [],[]\n",
    "\n",
    "    for i in range(len(data)-time_step-1):\n",
    "        a = data[i:(i+time_step),0]\n",
    "        x.append(a)\n",
    "        y.append(data[i+time_step,0])\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "time_step = 100 \n",
    "x, y = create_dataset(data, time_step)\n",
    "x = x.reshape(X.shape[0], x.shape[1],1)\n",
    "\n",
    "print(\"Shape of X:\", X.shape) \n",
    "print(\"Shape of Y:\", Y.shape) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d7c30e6-9269-4cb7-ab0f-d130913cc0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implemen multi head self attention \n",
    "\n",
    "class MultiHeadSelfAttention(Layer):\n",
    "\n",
    "    def __init__(self,embed_dim,num_head = 8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.projection_dim = embed_dim // num_heads \n",
    "        self.query_dense = Dense(embed_dim)\n",
    "        self.key_dense = Dense(embed_dim)\n",
    "        self.value_dense = Dense(embed_dim)\n",
    "        self.combine_heads = Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b = True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = ff.nn.softmax(scaled_score, axis =-1)\n",
    "        output = tf.matmul(weight, value)\n",
    "        return output, weights \n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm = [0,2,1,3])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        attention,_ = self.attention(query, batch_size)\n",
    "        attention = tf.transpose(attention, perm = [0,2,1,3])\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5795313d-d402-4498-bd74-3b01fcd8b7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
